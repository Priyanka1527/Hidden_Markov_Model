{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of our text file\n",
    "data = '../data/alllines.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Processing functions on the text !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quotation,comma etc punctuation marks using the corresponding string library\n",
    "def cleanText(line):\n",
    "    return line.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(dic, key, value):\n",
    "    if key not in dic:\n",
    "        dic[key] = []\n",
    "    dic[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProbabilityDict(list_text):\n",
    "    probability_dict = {}\n",
    "    list_len = len(list_text)\n",
    "    for item in list_text:\n",
    "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
    "    for key, value in probability_dict.items():\n",
    "        #Calculate probability and store\n",
    "        probability_dict[key] = value / list_len\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to hold the initial states and transitions\n",
    "first_word = {}\n",
    "second_word = {}\n",
    "transitions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train the Markov Model !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov model is a model following the Markov property where the next step depends only on the current step and not the past historical steps. The sequence of events following a Markov Model is called a Markov Chain. In this assignment, I've used a 2-nd order Markov Model where the current step depends on previous 2 states. The steps to approach the problem is as follows:\n",
    "\n",
    "1) Clean Up the Data\n",
    "2) Tokenize the text corpus\n",
    "3) Building the previous and current state-pairs\n",
    "4) Determine the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model():\n",
    "    for line in open(data):\n",
    "        \n",
    "        #Perform Tokenization-----\n",
    "        token = cleanText(line.rstrip().lower()).split()\n",
    "        tokenLength = len(token)\n",
    "        \n",
    "        #Building State-Pairs-----\n",
    "        for i in range(tokenLength):\n",
    "            curr_token = token[i]\n",
    "            \n",
    "            #For the first word, need to calculate initial state distribution\n",
    "            if i == 0:\n",
    "                first_word[curr_token] = first_word.get(curr_token, 0) + 1\n",
    "            else:\n",
    "                prev_token = token[i - 1]\n",
    "                \n",
    "                #For the last word, form additional identification token\n",
    "                if i == tokenLength - 1:\n",
    "                    makeDict(transitions, (prev_token, curr_token), 'END')\n",
    "                \n",
    "                #For the second word, consider it as 1st order Markov Model\n",
    "                if i == 1:\n",
    "                    makeDict(second_word, prev_token, curr_token)\n",
    "                else:\n",
    "                    prev2prev_token = token[i - 2]\n",
    "                    makeDict(transitions, (prev2prev_token, prev_token), curr_token)\n",
    "                    \n",
    "    # Building Probability Distribution\n",
    "    first_word_count = sum(first_word.values())\n",
    "    for key, value in first_word.items():\n",
    "        first_word[key] = value / first_word_count\n",
    "        \n",
    "    for prev, next_list in second_word.items():\n",
    "        second_word[prev] = makeProbabilityDict(next_list)\n",
    "        \n",
    "    for word_pair, next_list in transitions.items():\n",
    "        transitions[word_pair] = makeProbabilityDict(next_list)\n",
    "    \n",
    "    print('Training of the model is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model is done\n"
     ]
    }
   ],
   "source": [
    "#Call the function to train our model with the given text corpus\n",
    "markov_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating new text !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, we've First word distribution, Second word distribution and the State transition probaility distributios.\n",
    "To generate a new text, we'll refer to our above mentioned output and sample out from the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(dictionary):\n",
    "    num_rand = np.random.random()\n",
    "    cumulative = 0\n",
    "    for key, value in dictionary.items():\n",
    "        cumulative += value\n",
    "        if num_rand < cumulative:\n",
    "            return key\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to specify how long our newly generated text would be\n",
    "number_of_sentences = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sample text\n",
    "def generate_new_text():\n",
    "    for i in range(number_of_sentences):\n",
    "        sentence = []\n",
    "        \n",
    "        # First Word\n",
    "        word1 = sample_word(first_word)\n",
    "        sentence.append(word1)\n",
    "        # Second word\n",
    "        word2 = sample_word(second_word[word1])\n",
    "        sentence.append(word2)\n",
    "        \n",
    "        # Subsequent words untill END\n",
    "        while True:\n",
    "            word3 = sample_word(transitions[(word1, word2)])\n",
    "            if word3 == 'END':\n",
    "                break\n",
    "            sentence.append(word3)\n",
    "            word1 = word2\n",
    "            word2 = word3\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am the turned forth be it so\n",
      "enter paulina with a wicked lie\n",
      "if thou mayest discern by that small most greatly lived\n",
      "what remedy\n",
      "or in earth\n",
      "majesty bade me tell thee my lady to the ambassadors of england here\n",
      "and three times thrice sir\n",
      "feelingly now\n",
      "not to whisper wolseyhere makes visitation\n",
      "to lay my head\n"
     ]
    }
   ],
   "source": [
    "#Generating new text from given corpus\n",
    "generate_new_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hidden Markov Model, each state has a probability distribution over the possible output tokens (Emission Probability) in addition to the Transition probaility. Hence, a HMM consists of 2 information : 1) State Path and 2) Token Path(Emitted Sequence). The state path is not visible, we need to infer the underlying state path based on observable token path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment,the words in the corpus are hidden states, each input word is an observation. We need to predict what is going to be the completed sentence i.e. the next phrase based on the given phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(lines):\n",
    "        lines = cleanText(lines.rstrip().lower()).split()\n",
    "        word1 = lines[0]\n",
    "        if len(lines) == 1:\n",
    "            word2 = [max(self.second_word(word1), key=second_word(word1).get)]\n",
    "            lines.append(word2)\n",
    "        else:\n",
    "            word2 = lines[1]\n",
    "        while True:\n",
    "            word3 = max(transitions[(word1, word2)], key=transitions[(word1, word2)].get)\n",
    "            if word3 == 'END':\n",
    "                break\n",
    "            lines.append(word3)\n",
    "            word1 = word2\n",
    "            word2 = word3\n",
    "        print(' '.join(lines) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and breathe shortwinded accents of new broils.\n"
     ]
    }
   ],
   "source": [
    "#It gives the correct result from our actual corpus\n",
    "prediction(\"and breathe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach gives the correct result for the phrase \"and breathe\", Let's try another one and see what comes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter king henry vi queen margaret and ursula.\n"
     ]
    }
   ],
   "source": [
    "prediction(\"enter king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References: https://www.udemy.com/unsupervised-machine-learning-hidden-markov-models-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
