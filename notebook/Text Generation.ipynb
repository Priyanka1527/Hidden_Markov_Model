{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of our text file\n",
    "data = '../data/alllines.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Processing functions on the text !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quotation,comma etc punctuation marks using the corresponding string library\n",
    "def cleanText(line):\n",
    "    return line.translate(str.maketrans('','', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(dic, key, value):\n",
    "    if key not in dic:\n",
    "        dic[key] = []\n",
    "    dic[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeProbabilityDict(list_text):\n",
    "    probability_dict = {}\n",
    "    list_len = len(list_text)\n",
    "    for item in list_text:\n",
    "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
    "    for key, value in probability_dict.items():\n",
    "        #Calculate probability and store\n",
    "        probability_dict[key] = value / list_len\n",
    "    return probability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to hold the initial states and transitions\n",
    "first_word = {}\n",
    "second_word = {}\n",
    "transitions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train the Markov Model !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov model is a model following the Markov property where the next step depends only on the current step and not the past historical steps. The sequence of events following a Markov Model is called a Markov Chain. In this assignment, I've used a 2-nd order Markov Model where the current step depends on previous 2 states. The steps to approach the problem is as follows:\n",
    "\n",
    "1) Clean Up the Data\n",
    "2) Tokenize the text corpus\n",
    "3) Building the previous and current state-pairs\n",
    "4) Determine the probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model():\n",
    "    for line in open(data):\n",
    "        \n",
    "        #Perform Tokenization-----\n",
    "        token = cleanText(line.rstrip().lower()).split()\n",
    "        tokenLength = len(token)\n",
    "        \n",
    "        #Building State-Pairs-----\n",
    "        for i in range(tokenLength):\n",
    "            curr_token = token[i]\n",
    "            \n",
    "            #For the first word, need to calculate initial state distribution\n",
    "            if i == 0:\n",
    "                first_word[curr_token] = first_word.get(curr_token, 0) + 1\n",
    "            else:\n",
    "                prev_token = token[i - 1]\n",
    "                \n",
    "                #For the last word, form additional identification token\n",
    "                if i == tokenLength - 1:\n",
    "                    makeDict(transitions, (prev_token, curr_token), 'END')\n",
    "                \n",
    "                #For the second word, consider it as 1st order Markov Model\n",
    "                if i == 1:\n",
    "                    makeDict(second_word, prev_token, curr_token)\n",
    "                else:\n",
    "                    prev2prev_token = token[i - 2]\n",
    "                    makeDict(transitions, (prev2prev_token, prev_token), curr_token)\n",
    "                    \n",
    "    # Building Probability Distribution\n",
    "    first_word_count = sum(first_word.values())\n",
    "    for key, value in first_word.items():\n",
    "        first_word[key] = value / first_word_count\n",
    "        \n",
    "    for prev, next_list in second_word.items():\n",
    "        second_word[prev] = makeProbabilityDict(next_list)\n",
    "        \n",
    "    for word_pair, next_list in transitions.items():\n",
    "        transitions[word_pair] = makeProbabilityDict(next_list)\n",
    "    \n",
    "    print('Training of the model is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model is done\n"
     ]
    }
   ],
   "source": [
    "#Call the function to train our model with the given text corpus\n",
    "markov_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating new text !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, we've First word distribution, Second word distribution and the State transition probaility distributios.\n",
    "To generate a new text, we'll refer to our above mentioned output and sample out from the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(dictionary):\n",
    "    num_rand = np.random.random()\n",
    "    cumulative = 0\n",
    "    for key, value in dictionary.items():\n",
    "        cumulative += value\n",
    "        if num_rand < cumulative:\n",
    "            return key\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to specify how long our newly generated text would be\n",
    "number_of_sentences = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sample text\n",
    "def generate_new_text():\n",
    "    for i in range(number_of_sentences):\n",
    "        sentence = []\n",
    "        \n",
    "        # First Word\n",
    "        word1 = sample_word(first_word)\n",
    "        sentence.append(word1)\n",
    "        # Second word\n",
    "        word2 = sample_word(second_word[word1])\n",
    "        sentence.append(word2)\n",
    "        \n",
    "        # Subsequent words untill END\n",
    "        while True:\n",
    "            word3 = sample_word(transitions[(word1, word2)])\n",
    "            if word3 == 'END':\n",
    "                break\n",
    "            sentence.append(word3)\n",
    "            word1 = word2\n",
    "            word2 = word3\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooking both right and strength shall help to chop it off\n",
      "you are of monstrous friends nor has coriolanus\n",
      "two nights together\n",
      "and yet say not so good means as desire to a chaos or an unlickd bearwhelp\n",
      "swore he that is spoken fare you well\n",
      "strives bolingbroke to england will i draw\n",
      "why then the tribute which thou once didst bend against her maiden strewments and the kings to be\n",
      "nay master said\n",
      "let pity teach thee coz to shame it so\n",
      "i serve thee true\n"
     ]
    }
   ],
   "source": [
    "#Generating new text from given corpus\n",
    "generate_new_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References: https://www.udemy.com/unsupervised-machine-learning-hidden-markov-models-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
